# Transformer from Scratch for Financial Sentiment Analysis

This project implements a Transformer architecture **from scratch (excluding word embeddings)** for performing sentiment analysis on financial texts such as market news, stock discussions, and earnings reports.

## ğŸ” Objective

- Implement a configurable Transformer model for analyzing sentiment in financial data.
- Focus on low-level implementation: Multi-head attention, positional encoding, feed-forward layers, residuals, layer normalization, etc.
- Use **pretrained 1024-dimensional word embeddings** (e.g., GloVe, fastText, FinBERT). Word embedding creation is out of scope.

## ğŸ—ï¸ Features

- Manual control over Transformer architecture:
  - Number of layers
  - Number of attention heads
  - Custom `Wq`, `Wk`, `Wv` dimensions
  - Feedforward network size
  - Dropout rates and activation functions
- Sentiment classification head
- Clean modular design for easy experimentation

## ğŸ“ Project Structure

```
transformer-sentiment-finance/
â”œâ”€â”€ data/                    # Financial text datasets
â”œâ”€â”€ embeddings/              # Pretrained 1024-dim word embeddings (input only)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ transformer.py       # Core Transformer implementation
â”‚   â”œâ”€â”€ attention.py         # Multi-head self-attention from scratch
â”‚   â”œâ”€â”€ encoder.py           # Transformer encoder block
â”‚   â””â”€â”€ positional_encoding.py # Positional encoding implementation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_loader.py       # Data loading and preprocessing
â”‚   â”œâ”€â”€ metrics.py           # Accuracy, loss, confusion matrix
â”‚   â””â”€â”€ config.py            # Hyperparameter configuration
â”œâ”€â”€ train.py                 # Training loop
â”œâ”€â”€ evaluate.py              # Model evaluation and analysis
â”œâ”€â”€ config.json              # Model and training hyperparameters
â””â”€â”€ README.md                # Project overview
```

## ğŸ”§ Requirements

- Python 3.8+
- PyTorch
- NumPy
- scikit-learn
- pandas
- tqdm

Install with:

```bash
pip install -r requirements.txt
```

## ğŸ“Š Dataset (Planned)

- Financial PhraseBank
- FiQA Sentiment Dataset
- Market sentiment news datasets

## ğŸš€ Training

```bash
python train.py --config config.json
```

## ğŸ§  You Decide

You are responsible for:
- Selecting architecture dimensions (e.g., attention sizes, number of heads)
- Deciding model depth (layers)
- Choosing activation functions, optimizer, and learning schedule

## ğŸ“Œ TODO

- [ ] Build multi-head attention
- [ ] Add positional encoding
- [ ] Implement encoder block
- [ ] Build complete Transformer encoder
- [ ] Create training and evaluation loop
- [ ] Integrate with financial datasets
- [ ] Tune architecture and analyze results

---
